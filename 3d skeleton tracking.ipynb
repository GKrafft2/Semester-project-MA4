{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D skeleton tracking"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will create a 3d skeleton using OpenPifPaf to have the coordinates in the image coordinates and use the depth capacities of the intel Realsens d455 to add depth (3rd dimension)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: it's fairly important to use the included usb cable as others cables mignt not be recognized as usb 3.x. This is required in order to have everything working out smoothly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guillaume/anaconda3/envs/semproj/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import PIL\n",
    "import torch\n",
    "import openpifpaf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from timeit import timeit\n",
    "from time import time\n",
    "import os\n",
    "import cv2\n",
    "import pyrealsense2 as rs\n",
    "import math as m"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check version and available hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenPifPaf version 0.13.11\n",
      "PyTorch version 1.13.1\n"
     ]
    }
   ],
   "source": [
    "print('OpenPifPaf version', openpifpaf.__version__)\n",
    "print('PyTorch version', torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Openpifpaf predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = openpifpaf.Predictor(checkpoint='shufflenetv2k16') #could use mobilenetv3small for betterperformances"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Magic numbers and dicts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unit_vector(vector):\n",
    "    \"\"\" Returns the unit vector of the vector.  \"\"\"\n",
    "    return vector / np.linalg.norm(vector)\n",
    "\n",
    "def compute_angle(shoudler, hip, elbow):\n",
    "    \"\"\" Returns the angle in radiants between of the arm defined by its shoudler, hip and elbow\"\"\"\n",
    "    v1 = unit_vector(hip - shoudler)\n",
    "    v2 = unit_vector(elbow - shoudler)\n",
    "    return np.arccos(np.clip(np.dot(v1, v2),-1.0,1.0))*360/(2*np.pi)\n",
    "\n",
    "  \n",
    "def unit_vector(vector):\n",
    "    \"\"\" Returns the unit vector of the vector.  \"\"\"\n",
    "    return vector / np.linalg.norm(vector)\n",
    "\n",
    "def compute_angle(shoudler, hip, elbow):\n",
    "    \"\"\" Returns the angle in radiants between of the arm defined by its shoudler, hip and elbow\"\"\"\n",
    "    v1 = unit_vector(hip - shoudler)\n",
    "    v2 = unit_vector(elbow - shoudler)\n",
    "    return np.arccos(np.clip(np.dot(v1, v2),-1.0,1.0))*360/(2*np.pi)\n",
    "\n",
    "def compute_arms_angle(prediction):\n",
    "    \"\"\" Returns the angle of left and right arm wrt to the torso (return nan if the arm is not seen) \"\"\"\n",
    "    \n",
    "    pos_dict = {\n",
    "        \"nose\": 0,\n",
    "        \"left_eye\": 1,\n",
    "        \"right_eye\": 2,\n",
    "        \"left_ear\": 3,\n",
    "        \"right_ear\": 4,\n",
    "        \"left_shoudler\": 5,\n",
    "        \"right_shoudler\": 6,\n",
    "        \"left_elbow\": 7,\n",
    "        \"right_elbow\": 8,\n",
    "        \"left_wrist\": 9,\n",
    "        \"right_wrist\": 10,\n",
    "        \"left_hip\": 11,\n",
    "        \"right_hip\": 12,\n",
    "        \"left_knee\": 13,\n",
    "        \"right_knee\": 14,\n",
    "        \"left_ankle\": 15,\n",
    "        \"right_ankle\": 16}\n",
    "    \n",
    "    # we suppose only on person on the image (index = 0)\n",
    "    keypoints = predictions[0].data\n",
    "\n",
    "    # left arm\n",
    "    left_shoulder = keypoints[pos_dict[\"left_shoudler\"],0:2]\n",
    "    left_hip = keypoints[pos_dict[\"left_hip\"],0:2]\n",
    "    left_elbow = keypoints[pos_dict[\"left_elbow\"],0:2]\n",
    "\n",
    "    # right arm\n",
    "    right_shoulder = keypoints[pos_dict[\"right_shoudler\"],0:2]\n",
    "    right_hip = keypoints[pos_dict[\"right_hip\"],0:2]\n",
    "    right_elbow = keypoints[pos_dict[\"right_elbow\"],0:2]\n",
    "\n",
    "    return(compute_angle(left_shoulder, left_hip, left_elbow), compute_angle(right_shoulder, right_hip, right_elbow))\n",
    "\n",
    "def angles_to_command(left_angle, right_angle):\n",
    "    \"\"\"Returns the command corresponding to arms angle\"\"\"\n",
    "    \n",
    "    up_angle = 160\n",
    "    horizontal_angle = 90\n",
    "    down_angle = 20\n",
    "    margin = 15\n",
    "\n",
    "    # backward angle = 160 +/- 10\n",
    "    if (left_angle > up_angle-margin and left_angle < up_angle+margin) and (right_angle > up_angle-margin and right_angle < up_angle+margin):\n",
    "        return \"backward\"\n",
    "    # forward angle = 90 +/- 10\n",
    "    if (left_angle > horizontal_angle-margin and left_angle < horizontal_angle+margin) and (right_angle > horizontal_angle-margin and right_angle < horizontal_angle+margin):\n",
    "        return \"forward\"\n",
    "    # left = left: 90 +/- 10 right: 20 +/- 10\n",
    "    if (left_angle > horizontal_angle-margin and left_angle < horizontal_angle+margin) and (right_angle > down_angle-margin and right_angle < down_angle+margin):\n",
    "        return \"left\"\n",
    "    # right = left: 20 +/- 10 right: 90 +/- 10\n",
    "    if (left_angle > down_angle-margin and left_angle < down_angle+margin) and (right_angle > horizontal_angle-margin and right_angle < horizontal_angle+margin):\n",
    "        return \"right\"\n",
    "    else:\n",
    "        return \"stop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cart2sph(x,y,z):\n",
    "    XsqPlusYsq = x**2 + y**2\n",
    "    r = m.sqrt(XsqPlusYsq + z**2)               # r\n",
    "    elev = m.atan2(z,m.sqrt(XsqPlusYsq))     # theta\n",
    "    az = m.atan2(y,x)                           # phi\n",
    "    return r, elev, az\n",
    "\n",
    "def rad2deg(rad):\n",
    "    return rad*180/m.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "RED = (0, 0, 255)\n",
    "GREEN = (0, 255, 0)\n",
    "BLUE = (255, 0, 0)\n",
    "YELLOW = (0, 255, 255)\n",
    "\n",
    "pos_dict = {\n",
    "        \"nose\": 0,\n",
    "        \"left_eye\": 1,\n",
    "        \"right_eye\": 2,\n",
    "        \"left_ear\": 3,\n",
    "        \"right_ear\": 4,\n",
    "        \"left_shoulder\": 5,\n",
    "        \"right_shoulder\": 6,\n",
    "        \"left_elbow\": 7,\n",
    "        \"right_elbow\": 8,\n",
    "        \"left_wrist\": 9,\n",
    "        \"right_wrist\": 10,\n",
    "        \"left_hip\": 11,\n",
    "        \"right_hip\": 12,\n",
    "        \"left_knee\": 13,\n",
    "        \"right_knee\": 14,\n",
    "        \"left_ankle\": 15,\n",
    "        \"right_ankle\": 16}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = rs.pipeline()\n",
    "config = rs.config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get device product line for setting a supporting resolution\n",
    "pipeline_wrapper = rs.pipeline_wrapper(pipeline)\n",
    "pipeline_profile = config.resolve(pipeline_wrapper)\n",
    "device = pipeline_profile.get_device()\n",
    "device_product_line = str(device.get_info(rs.camera_info.product_line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyrealsense2.device: Intel RealSense D455 (S/N: 141322250607  FW: 05.14.00.00  on USB3.2)>\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_rgb = False\n",
    "for s in device.sensors:\n",
    "    if s.get_info(rs.camera_info.name) == 'RGB Camera':\n",
    "        found_rgb = True\n",
    "        break\n",
    "if not found_rgb:\n",
    "    print(\"The demo requires Depth camera with Color sensor\")\n",
    "    exit(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "\n",
    "if device_product_line == 'L500':\n",
    "    config.enable_stream(rs.stream.color, 960, 540, rs.format.bgr8, 30)\n",
    "else:\n",
    "    config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.2f}\".format(x)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3835/3859969795.py:14: RuntimeWarning: invalid value encountered in divide\n",
      "  return vector / np.linalg.norm(vector)\n"
     ]
    }
   ],
   "source": [
    "# Start streaming\n",
    "#pipeline.stop()\n",
    "cfg = pipeline.start(config)\n",
    "\n",
    "debug = False\n",
    "detection = True \n",
    "\n",
    "colorizer = rs.colorizer()\n",
    "\n",
    "profile = cfg.get_stream(rs.stream.depth) # Fetch stream profile for depth stream\n",
    "intr = profile.as_video_stream_profile().get_intrinsics() # Downcast to video_stream_profile and fetch intrinsics\n",
    "\n",
    "while True:\n",
    "\n",
    "    c = cv2.waitKey(1)\n",
    "    if c == 27: # press escape to quit\n",
    "        break\n",
    "\n",
    "    # Wait for a coherent pair of frames: depth and color\n",
    "    frames = pipeline.wait_for_frames()\n",
    "\n",
    "    # Create alignment primitive with color as its target stream:\n",
    "    align = rs.align(rs.stream.color)\n",
    "    frameset = align.process(frames)\n",
    "\n",
    "    # get aligned frames\n",
    "    depth_frame = frameset.get_depth_frame()\n",
    "    color_frame = frameset.get_color_frame()\n",
    "    if not depth_frame or not color_frame:\n",
    "        continue\n",
    "\n",
    "    # Convert images to numpy arrays\n",
    "    depth_image = np.asanyarray(depth_frame.get_data())\n",
    "    color_image = np.asanyarray(color_frame.get_data())\n",
    "    \n",
    "    \n",
    "    # ==== OPENPIFPAF =====\n",
    "    if detection:\n",
    "        # img = cv2.cvtColor(color_frame, cv2.COLOR_BGR2RGB)\n",
    "        pil_img = PIL.Image.fromarray(color_image)\n",
    "        frame = color_image\n",
    "\n",
    "        predictions, _, _ = predictor.pil_image(pil_img)\n",
    "\n",
    "        if debug:\n",
    "            print(f\"Angle of the left arm : {left_angle}\\nAngle of the right arm : {right_angle}\\n\")\n",
    "\n",
    "        try:\n",
    "            left_angle, right_angle = compute_arms_angle(predictions)\n",
    "\n",
    "            # writes desired command\n",
    "            command = angles_to_command(left_angle, right_angle)\n",
    "            # cv2.putText(img = frame, text=f\"command: {command}\", org = (0,60), fontFace=cv2.FONT_HERSHEY_TRIPLEX, fontScale=1, color=GREEN,thickness=3)\n",
    "            \n",
    "            # ==== skeleton drawing ====\n",
    "            #draw arms and hips\n",
    "            keypoints = predictions[0].data\n",
    "\n",
    "            # left arm\n",
    "            left_shoulder = keypoints[pos_dict[\"left_shoulder\"],0:2]\n",
    "            left_hip = keypoints[pos_dict[\"left_hip\"],0:2]\n",
    "            left_elbow = keypoints[pos_dict[\"left_elbow\"],0:2]\n",
    "            left_wrist = keypoints[pos_dict[\"left_wrist\"],0:2]\n",
    "\n",
    "            # right arm\n",
    "            right_shoulder = keypoints[pos_dict[\"right_shoulder\"],0:2]\n",
    "            right_hip = keypoints[pos_dict[\"right_hip\"],0:2]\n",
    "            right_elbow = keypoints[pos_dict[\"right_elbow\"],0:2]\n",
    "            right_wrist = keypoints[pos_dict[\"right_wrist\"],0:2]\n",
    "\n",
    "            # cv2.line(frame, tuple(map(int, tuple(left_shoulder))), tuple(map(int, tuple(left_elbow))), RED, 5)\n",
    "            # cv2.line(frame, tuple(map(int, tuple(left_shoulder))), tuple(map(int, tuple(left_hip))), YELLOW, 5)\n",
    "            # cv2.line(frame, tuple(map(int, tuple(right_shoulder))), tuple(map(int, tuple(right_elbow))), RED, 5)\n",
    "            # cv2.line(frame, tuple(map(int, tuple(right_shoulder))), tuple(map(int, tuple(right_hip))), YELLOW, 5)\n",
    "\n",
    "            cv2.line(frame, tuple(map(int, tuple(right_elbow))), tuple(map(int, tuple(right_wrist))), RED, 5)\n",
    "            # display_whole_skeleton(frame, keypoints)\n",
    "            # ==== END skeleton drawing ====\n",
    "\n",
    "            # get 3d coordinates of the left and right elbow\n",
    "            left_elbow_3d = rs.rs2_deproject_pixel_to_point(intr, left_elbow, depth_frame.get_distance(int(left_elbow[0]), int(left_elbow[1])))\n",
    "            right_elbow_3d = rs.rs2_deproject_pixel_to_point(intr, right_elbow, depth_frame.get_distance(int(right_elbow[0]), int(right_elbow[1])))\n",
    "\n",
    "            #get 3d coordinates of the left and right wrists\n",
    "            left_wrist_3d = rs.rs2_deproject_pixel_to_point(intr, left_wrist, depth_frame.get_distance(int(left_wrist[0]), int(left_wrist[1])))\n",
    "            right_wrist_3d = rs.rs2_deproject_pixel_to_point(intr, right_wrist, depth_frame.get_distance(int(right_wrist[0]), int(right_wrist[1])))\n",
    "\n",
    "            # compute vector from elbow to wrist\n",
    "            left_forearm_vector = np.array(left_wrist_3d) - np.array(left_elbow_3d)\n",
    "            right_forearm_vector = np.array(right_wrist_3d) - np.array(right_elbow_3d)\n",
    "\n",
    "            right_forearm_vector[[1,2]] = right_forearm_vector[[2,1]] # swap y and z\n",
    "            right_forearm_vector[2] = -right_forearm_vector[2] # invert z\n",
    "\n",
    "            # compute length of the forearm\n",
    "            left_forearm_length = np.linalg.norm(left_forearm_vector)\n",
    "            right_forearm_length = np.linalg.norm(right_forearm_vector)\n",
    "\n",
    "            # convert to spherical coordinates\n",
    "            _, elevation, azimuth = cart2sph(*right_forearm_vector)\n",
    "            elevation = rad2deg(elevation)\n",
    "            azimuth = rad2deg(azimuth)\n",
    "\n",
    "            cv2.putText(img = frame, text=f\"elevation: {elevation:.1f}, azimuth: {azimuth:.1f}\", org = (0,60), fontFace=cv2.FONT_HERSHEY_TRIPLEX, fontScale=0.5, color=GREEN,thickness=1)\n",
    "            cv2.putText(img = frame, text=f\"right forearm vector: {right_forearm_vector}\", org = (0,120), fontFace=cv2.FONT_HERSHEY_TRIPLEX, fontScale=0.5, color=GREEN,thickness=1)\n",
    "\n",
    "            # print(f\"elevation: {rad2deg(elevation)}, azimuth: {rad2deg(azimuth)}\", end = \"\\r\")\n",
    "            # print(f\"right forearm vector: {right_forearm_vector}\", end = \"\\r\")\n",
    "\n",
    "        except:\n",
    "            cv2.putText(img = frame, text=f\"no detection\", org = (0,60), fontFace=cv2.FONT_HERSHEY_TRIPLEX, fontScale=1, color=GREEN,thickness=2)\n",
    "\n",
    "    # ==== END OPENPIFPAF =====\n",
    "    \n",
    "    # Update color and depth frames:\n",
    "    aligned_depth_frame = frameset.get_depth_frame()\n",
    "    colorized_depth = np.asanyarray(colorizer.colorize(aligned_depth_frame).get_data())\n",
    "\n",
    "    # Show the two frames together:\n",
    "    images = np.hstack((color_image, colorized_depth))\n",
    "    # plt.imshow(images)\n",
    "\n",
    "    # Show images\n",
    "    cv2.namedWindow('RealSense', cv2.WINDOW_NORMAL)\n",
    "    cv2.imshow('RealSense', images)\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "# Stop streaming\n",
    "pipeline.stop()\n",
    "\n",
    "# Release resources\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs.deproject_pixel_to_point()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "6a1eca162d8244ec663334057c1610a3bae01391a28d85af84dac413dee83203"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
