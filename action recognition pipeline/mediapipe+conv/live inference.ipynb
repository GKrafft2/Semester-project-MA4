{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import time \n",
    "import pyrealsense2 as rs\n",
    "import math as m\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_landmark_to_array(landmarks):\n",
    "    landmarks_array = np.array(\n",
    "        [[landmark.x, landmark.y, landmark.z] for landmark in landmarks])\n",
    "    return landmarks_array"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, n_input_channels=3, n_output=4):\n",
    "        super().__init__()\n",
    "    \n",
    "        # input = 75x20x3\n",
    "        self.conv1 = nn.Conv2d(n_input_channels, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        # input = 37x10x32\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # input = 18x5x64\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # input = 9x2x128\n",
    "        self.fc1 = nn.Linear(128*9*2, 512)\n",
    "        self.fc2 = nn.Linear(512, n_output)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.max_pool2d(x, 2) #maxpool of kernel size 2 to reduce the size of the images by a factor 2\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(-1, 128*9*2) #flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def predict(self, x):\n",
    "        logits = self.forward(x)\n",
    "        return F.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "CHECKPOINT = \"trained_model.pth\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = ConvNet().to(device)\n",
    "model.load_state_dict(torch.load(CHECKPOINT))\n",
    "model.eval()\n",
    "\n",
    "# useful for normalization\n",
    "dataset_mean = np.load(\"dataset_mean.npy\")\n",
    "dataset_std = np.load(\"dataset_std.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.50957664,  0.58962419, -0.06745016])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(data):\n",
    "    new_data = np.zeros(data.shape)\n",
    "    new_data[:,:,0] = (data[:,:,0] - dataset_mean[0])/dataset_std[0]\n",
    "    new_data[:,:,1] = (data[:,:,1] - dataset_mean[1])/dataset_std[1]\n",
    "    new_data[:,:,2] = (data[:,:,2] - dataset_mean[2])/dataset_std[2]\n",
    "    return new_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {\"come\":0,\"lie down\":1,\"sit\":2,\"stay\":3}\n",
    "inv_label_dict = {v: k for k, v in label_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 59\u001b[0m\n\u001b[1;32m     57\u001b[0m data \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39munsqueeze(data, \u001b[39m0\u001b[39m)\n\u001b[1;32m     58\u001b[0m \u001b[39m# print(data.shape)\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m     61\u001b[0m \u001b[39m# predict\u001b[39;00m\n\u001b[1;32m     62\u001b[0m output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(data)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "new_frame_time = 0\n",
    "prev_frame_time = 0\n",
    "\n",
    "predicted_class_name = \"hee\"\n",
    "\n",
    "\n",
    "# could find a way of finding this value automatically\n",
    "frames_by_sec = 20 - 1\n",
    "n_frames = 0\n",
    "landmark_image = np.empty((75,3))\n",
    "\n",
    "#hollistic detects posiiton, face and hand landmarks\n",
    "mp_holistic = mp.solutions.holistic\n",
    "holistic_model = mp_holistic.Holistic(\n",
    "    model_complexity=1,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "moves = [\"sit\", \"lie down\", \"stay\", \"come\"]\n",
    "\n",
    "\n",
    "n_samples = 30 # number of times we will record each move\n",
    "current_sample = 0 # current sample we are recording\n",
    "current_move = 0\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "cv2.namedWindow(\"image\", cv2.WINDOW_NORMAL)\n",
    "while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "      print(\"Ignoring empty camera frame.\")\n",
    "      # If loading a video, use 'break' instead of 'continue'.\n",
    "      continue\n",
    "\n",
    "    n_frames += 1\n",
    "\n",
    "    if n_frames > 20:\n",
    "      if n_frames % 20 == 0:\n",
    "        # convert to tensor\n",
    "        data = landmark_image[:,:,-20:]\n",
    "        data = np.transpose(data, (0, 2, 1))\n",
    "        data = np.transpose(data, (2, 0, 1))\n",
    "        data = np.nan_to_num(data)\n",
    "        data = np.clip(data, -10, 10)\n",
    "        # print(\"before normalization\")\n",
    "        # print(data)\n",
    "        data = normalize_data(data)\n",
    "        # print(\"after normalization\")\n",
    "        # print(data)\n",
    "        data = torch.tensor(data).float() \n",
    "        # print(data.shape)\n",
    "        # data = data.transpose(1,0,2)\n",
    "        data = torch.unsqueeze(data, 0)\n",
    "        # print(data.shape)\n",
    "        data = data.to(device)\n",
    "\n",
    "        # predict\n",
    "        output = model.predict(data)\n",
    "        print(f\"frame {n_frames}\")\n",
    "        print(output)\n",
    "        # get the predicted class\n",
    "        if torch.max(output) > 0.6:\n",
    "          predicted_class = torch.argmax(output, dim=1)\n",
    "          predicted_class_name = inv_label_dict[int(predicted_class)]\n",
    "\n",
    "    # compute fps\n",
    "    new_frame_time = time.time()\n",
    "    fps = 1/(new_frame_time-prev_frame_time)\n",
    "    prev_frame_time = new_frame_time\n",
    "\n",
    "    # compute keypoints etc...\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    image.flags.writeable = False\n",
    "    results = holistic_model.process(image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # convert landmarks to array with exception handling (zero array if not detected)\n",
    "    try:\n",
    "      pose_landmarks = convert_landmark_to_array(results.pose_landmarks.landmark)\n",
    "    except:\n",
    "      #  print(\"no pose landmarks detected\")\n",
    "       pose_landmarks = np.zeros((33,3))\n",
    "    try:\n",
    "      left_hand_landmarks = convert_landmark_to_array(results.left_hand_landmarks.landmark)\n",
    "    except:\n",
    "      # print(\"no left hand landmarks detected\")\n",
    "      left_hand_landmarks = np.zeros((21,3))\n",
    "    try:\n",
    "      right_hand_landmarks = convert_landmark_to_array(results.right_hand_landmarks.landmark)\n",
    "    except:\n",
    "      # print(\"no right hand landmarks detected\")\n",
    "      right_hand_landmarks = np.zeros((21,3))\n",
    "\n",
    "    hands_and_pose_array = np.concatenate((pose_landmarks, left_hand_landmarks, right_hand_landmarks), axis=0)\n",
    "    landmark_image = np.dstack((landmark_image, hands_and_pose_array))\n",
    "\n",
    "    # Drawing the annotiations\n",
    "    image.flags.writeable = True\n",
    "    \n",
    "    # Drawing Pose Landmarks\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.pose_landmarks,\n",
    "        mp_holistic.POSE_CONNECTIONS,\n",
    "        landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())\n",
    "    \n",
    "    # Drawing Right hand Landmarks\n",
    "    mp_drawing.draw_landmarks(\n",
    "      image,\n",
    "      results.right_hand_landmarks,\n",
    "      mp_holistic.HAND_CONNECTIONS\n",
    "    )\n",
    " \n",
    "    # Drawing Left hand Landmarks\n",
    "    mp_drawing.draw_landmarks(\n",
    "      image,\n",
    "      results.left_hand_landmarks,\n",
    "      mp_holistic.HAND_CONNECTIONS\n",
    "    )\n",
    "\n",
    "    fps = str(int(fps))\n",
    "    image = cv2.flip(image,1)\n",
    "    cv2.putText(image, fps, (7, 70), 1, 3, (100, 255, 0), 3, cv2.LINE_AA)\n",
    "    cv2.putText(image, predicted_class_name, (7, 100), 1, 3, (100, 255, 0), 3, cv2.LINE_AA)\n",
    "    cv2.imshow('image', image)\n",
    "    c = cv2.waitKey(1)\n",
    "    if c == 27: # press escape to quit\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.59088641  0.59006107  0.5900647  ...  0.52256835  0.51002502\n",
      "    0.50510854]\n",
      "  [ 0.21145545  0.21115653  0.21099502 ... -0.20798609 -0.14849935\n",
      "   -0.12320651]\n",
      "  [-2.040483   -2.04355097 -2.01860261 ... -0.75773156 -0.56069994\n",
      "   -0.59763104]]\n",
      "\n",
      " [[ 0.64257628  0.64151841  0.64150804 ...  0.54083884  0.52911341\n",
      "    0.52235234]\n",
      "  [ 0.11997752  0.11982448  0.11897407 ... -0.24453324 -0.18421206\n",
      "   -0.16392471]\n",
      "  [-2.00935268 -2.01289988 -1.98904061 ... -0.77320743 -0.55757856\n",
      "   -0.58639228]]\n",
      "\n",
      " [[ 0.67216563  0.67118281  0.67122722 ...  0.55443913  0.54195845\n",
      "    0.53246504]\n",
      "  [ 0.1141978   0.11400199  0.11259723 ... -0.23271029 -0.17502123\n",
      "   -0.15437298]\n",
      "  [-2.00990438 -2.0134387  -1.98953438 ... -0.77323598 -0.55775183\n",
      "   -0.58658379]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.          0.          0.         ...  0.          0.\n",
      "    0.1895048 ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    1.04024053]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.00818705]]\n",
      "\n",
      " [[ 0.          0.          0.         ...  0.          0.\n",
      "    0.20902383]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    1.04749906]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.00656147]]\n",
      "\n",
      " [[ 0.          0.          0.         ...  0.          0.\n",
      "    0.22230725]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    1.04639637]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.00651312]]]\n"
     ]
    }
   ],
   "source": [
    "print(landmark_image[:,:,40:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.set_printoptions(threshold=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
