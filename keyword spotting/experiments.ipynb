{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword spotting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Interesting Article about audio](https://www.seeedstudio.com/blog/2018/11/23/6-important-speech-recognition-technology-you-need-to-know/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keyword spotting consits of detecting a limited set of keywords, this is typically what is used to wake up IoT devices (Alexa etc.). One of the datasets available is called [Google Speech Commands](https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html) here his the [related paper](https://arxiv.org/abs/1804.03209). This is the one we are going to focus on. It contains a list of 35 words for a total  of 105'829 utterances. They were recored by different users all using their phone or laptop mic (the data was collected using a web application). The dataset also contains backgrouind noise audio (see \"_background_noise_\" folder), because it is important to be bale to distinguish audio that contains speech from audio that contains none.\n",
    "\n",
    "Here is the list of words and the number of occurences:\n",
    "\n",
    "![List of keywords](images/Capture.PNG)\n",
    "\n",
    "For the project we could use several of those keywords for the robot to understand. We use the V2 version of the dataset.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sidenode a framework called [fairseq](https://github.com/facebookresearch/fairseq) could be used for more complex speech recognition task. It is very popular (+20k stars on github)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the model implemented in this [paper](https://arxiv.org/ftp/arxiv/papers/2101/2101.04792.pdf) as it has the best SOTA results on [papers with code](https://paperswithcode.com/sota/keyword-spotting-on-google-speech-commands ) on our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Res15(nn.Module):\n",
    "    def __init__(self, n_maps):\n",
    "        super(Res15, self).__init__()\n",
    "        n_maps = n_maps\n",
    "        self.conv0 = nn.Conv2d(1, n_maps, (3, 3), padding=(1, 1), bias=False)\n",
    "        self.n_layers = n_layers = 13\n",
    "        dilation = True\n",
    "        if dilation:\n",
    "            self.convs = [nn.Conv2d(n_maps, n_maps, (3, 3), padding=int(2 ** (i // 3)), dilation=int(2 ** (i // 3)),\n",
    "                                    bias=False) for i in range(n_layers)]\n",
    "        else:\n",
    "            self.convs = [nn.Conv2d(n_maps, n_maps, (3, 3), padding=1, dilation=1,\n",
    "                                    bias=False) for _ in range(n_layers)]\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            self.add_module(\"bn{}\".format(i + 1), nn.BatchNorm2d(n_maps, affine=False))\n",
    "            self.add_module(\"conv{}\".format(i + 1), conv)\n",
    "\n",
    "    def forward(self, audio_signal, length=None):\n",
    "        x = audio_signal.unsqueeze(1)\n",
    "        for i in range(self.n_layers + 1):\n",
    "            y = F.relu(getattr(self, \"conv{}\".format(i))(x))\n",
    "            if i == 0:\n",
    "                if hasattr(self, \"pool\"):\n",
    "                    y = self.pool(y)\n",
    "                old_x = y\n",
    "            if i > 0 and i % 2 == 0:\n",
    "                x = y + old_x\n",
    "                old_x = x\n",
    "            else:\n",
    "                x = y\n",
    "            if i > 0:\n",
    "                x = getattr(self, \"bn{}\".format(i))(x)\n",
    "        x = x.view(x.size(0), x.size(1), -1)  # shape: (batch, feats, o3)\n",
    "        x = torch.mean(x, 2)\n",
    "        return x.unsqueeze(-2), length"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using HuggingFace"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/superb/wav2vec2-base-superb-ks\n",
    "seems to be a popular option\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is trained on speech commands v1 it contains the 10 following words:\n",
    "1. Yes\n",
    "2. No\n",
    "3. Up\n",
    "4. Down\n",
    "5. Left\n",
    "6. Right\n",
    "7. On\n",
    "8. Off\n",
    "9. Stop\n",
    "10. Go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m load_dataset\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m pipeline\n\u001b[0;32m      4\u001b[0m dataset \u001b[39m=\u001b[39m load_dataset(\u001b[39m\"\u001b[39m\u001b[39manton-l/superb_demo\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mks\u001b[39m\u001b[39m\"\u001b[39m, split\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "\n",
    "dataset = load_dataset(\"anton-l/superb_demo\", \"ks\", split=\"test\")\n",
    "\n",
    "classifier = pipeline(\"audio-classification\", model=\"superb/wav2vec2-base-superb-ks\")\n",
    "labels = classifier(dataset[0][\"file\"], top_k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.9999943971633911, 'label': '_silence_'}, {'score': 2.4985183699755e-06, 'label': 'left'}, {'score': 1.984544496735907e-06, 'label': 'down'}, {'score': 4.3534416249713104e-07, 'label': '_unknown_'}, {'score': 3.17640683533682e-07, 'label': 'stop'}]\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/guillaume/.cache/huggingface/datasets/downloads/extracted/8b845bff8b050c19206f97a59ed3450967d8cd6f93823158ae48dae62e8c2041/_silence_/5.wav'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][\"file\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "CHUNK = 320  # number of audio samples per frame\n",
    "FORMAT = pyaudio.paInt16  # audio format\n",
    "CHANNELS = 1  # mono audio\n",
    "RATE = 16000  # sampling rate in Hz\n",
    "RECORD_SECONDS = 0.5  # duration of each recording in seconds\n",
    "FILE_NAME = f\"temp.wav\"\n",
    "\n",
    "def record_audio():\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    stream = p.open(format=FORMAT,\n",
    "                    channels=CHANNELS,\n",
    "                    rate=RATE,\n",
    "                    input=True,\n",
    "                    frames_per_buffer=CHUNK)\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            frames = []  # to store audio frames\n",
    "\n",
    "            for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "                data = stream.read(CHUNK)\n",
    "                frames.append(data)\n",
    "\n",
    "            # write frames to temporary WAV file\n",
    "            \n",
    "            wav_filename =  FILE_NAME\n",
    "            wf = wave.open(wav_filename, 'wb')\n",
    "            wf.setnchannels(CHANNELS)\n",
    "            wf.setsampwidth(p.get_sample_size(FORMAT))\n",
    "            wf.setframerate(RATE)\n",
    "            wf.writeframes(b''.join(frames))\n",
    "            wf.close()\n",
    "\n",
    "            # read contents of WAV file a\n",
    "\n",
    "            yield wav_filename\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.9989523887634277, 'label': 'yes'}]\n",
      "[{'score': 0.9999964237213135, 'label': 'go'}]\n",
      "[{'score': 0.9923818707466125, 'label': 'off'}]\n",
      "[{'score': 0.999963641166687, 'label': 'yes'}]\n",
      "[{'score': 0.999875545501709, 'label': 'no'}]\n",
      "[{'score': 0.9981778860092163, 'label': 'off'}]\n",
      "[{'score': 0.9991490840911865, 'label': 'yes'}]\n",
      "[{'score': 0.9961571097373962, 'label': 'yes'}]\n",
      "[{'score': 0.9999779462814331, 'label': 'no'}]\n",
      "[{'score': 0.9993952512741089, 'label': 'no'}]\n",
      "[{'score': 0.9999998807907104, 'label': 'yes'}]\n",
      "[{'score': 0.9973733425140381, 'label': '_unknown_'}]\n",
      "[{'score': 0.9984136819839478, 'label': 'no'}]\n",
      "[{'score': 0.9907177090644836, 'label': 'go'}]\n",
      "[{'score': 1.0, 'label': 'yes'}]\n",
      "No keyword detected\r"
     ]
    }
   ],
   "source": [
    "for wav_data in record_audio():\n",
    "    # pass the WAV data to your keyword spotter here\n",
    "    label = classifier(wav_data, top_k=1)\n",
    "    if label[0][\"score\"] > 0.99:\n",
    "        print(label)\n",
    "    else:\n",
    "        print(\"No keyword detected\", end = \"\\r\")\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6a1eca162d8244ec663334057c1610a3bae01391a28d85af84dac413dee83203"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
